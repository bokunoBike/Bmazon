目 录 
第 1 章 引言 . . . . . . . . . 1 
1.1 本书面向的读者 . . . . . . . . . . . . . . . . . . . .7 
1.2 深度学习的历史趋势 . . . . . . . . . . . . . . . 8 
1.2.1 神经网络的众多名称和命运变迁 . 8 
1.2.2 与日俱增的数据量 . . . . . . . . . . . . . 12 
1.2.3 与日俱增的模型规模 . . . . . . . . . . .13 
1.2.4 与日俱增的精度、复杂度和对现实世界的冲击 . . . . . . . . . . . . . 15

第 1 部分 应用数学与机器学习基础 
第 2 章 线性代数 . . . 19 
2.1 标量、向量、矩阵和张量 . . . . . . . . . . 19 
2.2 矩阵和向量相乘. . . . . . . . . . . . . . . . . . .21 
2.3 单位矩阵和逆矩阵 . . . . . . . . . . . . . . . . 22 
2.4 线性相关和生成子空间 . . . . . . . . . . . 23 
2.5 范数. . . . . . . . .24 
2.6 特殊类型的矩阵和向量 . . . . . . . . . . . 25 
2.7 特征分解 . . . . 26 
2.8 奇异值分解 . . . . . . . . . . . . . . . . . . . . . . . 28 
2.9 Moore-Penrose 伪逆 . . . . . . . . . . . . . . . 28 
2.10 迹运算 . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 
2.11 行列式 . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 
2.12 实例：主成分分析. . . . . . . . . . . . . . . .30 
第 3 章 概率与信息论. . . . . . . . . . . . . . . . . . . . .34 
3.1 为什么要使用概率 . . . . . . . . . . . . . . . . 34 
3.2 随机变量 . . . . . . . . . . . . . . . . . . . . . . . . . . 35 
3.3 概率分布 . . . . . . . . . . . . . . . . . . . . . . . . . . 36 
3.3.1 离散型变量和概率质量函数 . . . . 36 
3.3.2 连续型变量和概率密度函数 . . . . 36 
3.4 边缘概率 . . . . . . . . . . . . . . . . . . . . . . . . . . 37 
3.5 条件概率 . . . . . . . . . . . . . . . . . . . . . . . . . . 37 
3.6 条件概率的链式法则 . . . . . . . . . . . . . . 38
3.7 独立性和条件独立性 . . . . . . . . . . . . . . 38 
3.8 期望、方差和协方差 . . . . . . . . . . . . . . 38 
3.9 常用概率分布 . . . . . . . . . . . . . . . . . . . . . 39 
3.9.1 Bernoulli 分布 . . . . . . . . . . . . . . . . 40 
3.9.2 Multinoulli 分布 . . . . . . . . . . . . . . 40 
3.9.3 高斯分布 . . . . . . . . . . . . . . . . . . . . . 40 
3.9.4 指数分布和 Laplace 分布 . . . . . . 41 
3.9.5 Dirac 分布和经验分布 . . . . . . . . . 42 
3.9.6 分布的混合 . . . . . . . . . . . . . . . . . . . 42 
3.10 常用函数的有用性质. . . . . . . . . . . . .43 
3.11 贝叶斯规则 . . . . . . . . . . . . . . . . . . . . . . 45 
3.12 连续型变量的技术细节 . . . . . . . . . . 45 
3.13 信息论 . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 
3.14 结构化概率模型 . . . . . . . . . . . . . . . . . 49 
第 4 章 数值计算 . . . . . . . . . . . . . . . . . . . . . . . . . 52 
4.1 上溢和下溢 . . . . . . . . . . . . . . . . . . . . . . . 52 
4.2 病态条件 . . . . . . . . . . . . . . . . . . . . . . . . . . 53 
4.3 基于梯度的优化方法 . . . . . . . . . . . . . . 53 
4.3.1 梯度之上：Jacobian 和 Hessian 矩阵 . . . . . . . . . . . . . . . . . . . . . 56 
4.4 约束优化 . . . . . . . . . . . . . . . . . . . . . . . . . . 60 
4.5 实例：线性最小二乘 . . . . . . . . . . . . . . 61 
第 5 章 机器学习基础. . . . . . . . . . . . . . . . . . . . .63 
5.1 学习算法 . . . . . . . . . . . . . . . . . . . . . . . . . . 63 
5.1.1 任务 T . . . . . . . . . . . . . . . . . . . . . . 63 
5.1.2 性能度量 P . . . . . . . . . . . . . . . . . . . 66 
5.1.3 经验 E . . . . . . . . . . . . . . . . . . . . . . . 66 
5.1.4 示例：线性回归 . . . . . . . . . . . . . . . 68 
5.2 容量、过拟合和欠拟合 . . . . . . . . . . . . 70 
5.2.1 没有免费午餐定理 . . . . . . . . . . . . . 73 
5.2.2 正则化 . . . . . . . . . . . . . . . . . . . . . . . 74 
5.3 超参数和验证集. . . . . . . . . . . . . . . . . . .76 
5.3.1 交叉验证 . . . . . . . . . . . . . . . . . . . . . 76 
5.4 估计、偏差和方差. . . . . . . . . . . . . . . . .77 
5.4.1 点估计 . . . . . . . . . . . . . . . . . . . . . . . 77 
5.4.2 偏差 . . . . . . . . . . . . . . . . . . . . . . . . . 78 
5.4.3 方差和标准差 . . . . . . . . . . . . . . . . . 80 
5.4.4 权衡偏差和方差以最小化均方误差 . . . . . . . . . . . . . . . . . . . . . . . 81 
5.4.5 一致性 . . . . . . . . . . . . . . . . . . . . . . . 82 
5.5 最 大似然估计 . . . . . . . . . . . . . . . . . . . . . 82 
5.5.1 条件对数似然和均方误差. . . . . . .84
5.5.2 最 大似然的性质 . . . . . . . . . . . . . . . 84 
5.6 贝叶斯统计 . . . . . . . . . . . . . . . . . . . . . . . 85 
5.6.1 最 大后验 (MAP) 估计 . . . . . . . . . 87 
5.7 监督学习算法 . . . . . . . . . . . . . . . . . . . . . 88 
5.7.1 概率监督学习 . . . . . . . . . . . . . . . . . 88 
5.7.2 支持向量机 . . . . . . . . . . . . . . . . . . . 88 
5.7.3 其他简单的监督学习算法. . . . . . .90 
5.8 无监督学习算法. . . . . . . . . . . . . . . . . . .91 
5.8.1 主成分分析 . . . . . . . . . . . . . . . . . . . 92 
5.8.2 k-均值聚类 . . . . . . . . . . . . . . . . . . . .94 
5.9 随机梯度下降 . . . . . . . . . . . . . . . . . . . . . 94 
5.10 构建机器学习算法 . . . . . . . . . . . . . . . 96 
5.11 促使深度学习发展的挑战 . . . . . . . . 96 
5.11.1 维数灾难 . . . . . . . . . . . . . . . . . . . . 97 
5.11.2 局部不变性和平滑正则化 . . . . . 97 
5.11.3 流形学习 . . . . . . . . . . . . . . . . . . . . 99 

第 2 部分 深度网络：现代实践 
第 6 章 深度前馈网络 . . . . . . . . . . . . . . . . . . . 105 
6.1 实例：学习 XOR. . . . . . . . . . . . . . . . . 107 
6.2 基于梯度的学习 . . . . . . . . . . . . . . . . . 110 
6.2.1 代价函数 . . . . . . . . . . . . . . . . . . . . 111 
6.2.2 输出单元 . . . . . . . . . . . . . . . . . . . . 113 
6.3 隐藏单元. . . . . . . . . . . . . . . . . . . . . . . . .119 
6.3.1 整流线性单元及其扩展 . . . . . . . 120 
6.3.2 logistic sigmoid 与双曲正切函数 . . . . . . . . . . . . . . . . . . . . . . . . 121 
6.3.3 其他隐藏单元 . . . . . . . . . . . . . . . . 122 
6.4 架构设计. . . . . . . . . . . . . . . . . . . . . . . . .123 
6.4.1 万能近似性质和深度. . . . . . . . . .123 
6.4.2 其他架构上的考虑 . . . . . . . . . . . .126 
6.5 反向传播和其他的微分算法. . . . . .126 
6.5.1 计算图 . . . . . . . . . . . . . . . . . . . . . . 127 
6.5.2 微积分中的链式法则. . . . . . . . . .128 
6.5.3 递归地使用链式法则来实现反向传播 . . . . . . . . . . . . . . . . . . . . 128 
6.5.4 全连接 MLP 中的反向传播计算 . . . . . . . . . . . . . . . . . . . . . . . . 131 
6.5.5 符号到符号的导数 . . . . . . . . . . . .131 
6.5.6 一般化的反向传播 . . . . . . . . . . . .133 
6.5.7 实例：用于 MLP 训练的反向传播 . . . . . . . . . . . . . . . . . . . . . . .135 
6.5.8 复杂化 . . . . . . . . . . . . . . . . . . . . . . 137
6.5.9 深度学习界以外的微分 . . . . . . . 137 
6.5.10 高阶微分 . . . . . . . . . . . . . . . . . . . 138 
6.6 历史小记. . . . . . . . . . . . . . . . . . . . . . . . .139 
第 7 章 深度学习中的正则化 . . . . . . . . . . . . 141 
7.1 参数范数惩罚 . . . . . . . . . . . . . . . . . . . . 142 
7.1.1 L2 参数正则化 . . . . . . . . . . . . . . . 142 
7.1.2 L1 正则化 . . . . . . . . . . . . . . . . . . . 144 
7.2 作为约束的范数惩罚. . . . . . . . . . . . .146 
7.3 正则化和欠约束问题. . . . . . . . . . . . .147 
7.4 数据集增强 . . . . . . . . . . . . . . . . . . . . . . 148 
7.5 噪声鲁棒性 . . . . . . . . . . . . . . . . . . . . . . 149 
7.5.1 向输出目标注入噪声. . . . . . . . . .150 
7.6 半监督学习 . . . . . . . . . . . . . . . . . . . . . . 150 
7.7 多任务学习 . . . . . . . . . . . . . . . . . . . . . . 150 
7.8 提前终止. . . . . . . . . . . . . . . . . . . . . . . . .151 
7.9 参数绑定和参数共享. . . . . . . . . . . . .156 
7.9.1 卷积神经网络 . . . . . . . . . . . . . . . . 156 
7.10 稀疏表示. . . . . . . . . . . . . . . . . . . . . . . .157 
7.11 Bagging 和其他集成方法. . . . . . . .158 
7.12 Dropout . . . . . . . . . . . . . . . . . . . . . . . . .159 
7.13 对抗训练. . . . . . . . . . . . . . . . . . . . . . . .165 
7.14 切面距离、正切传播和流形正切分类器 . . . . . . . . . . . . . . . . . . 167 
第 8 章 深度模型中的优化. . . . . . . . . . . . . . .169 
8.1 学习和纯优化有什么不同 . . . . . . . . 169 
8.1.1 经验风险最小化 . . . . . . . . . . . . . . 169 
8.1.2 代理损失函数和提前终止 . . . . . 170 
8.1.3 批量算法和小批量算法 . . . . . . . 170 
8.2 神经网络优化中的挑战 . . . . . . . . . . 173 
8.2.1 病态 . . . . . . . . . . . . . . . . . . . . . . . . 173 
8.2.2 局部极小值 . . . . . . . . . . . . . . . . . . 174 
8.2.3 高原、鞍点和其他平坦区域 . . . .175 
8.2.4 悬崖和梯度爆炸 . . . . . . . . . . . . . . 177 
8.2.5 长期依赖 . . . . . . . . . . . . . . . . . . . . 177 
8.2.6 非精确梯度 . . . . . . . . . . . . . . . . . . 178 
8.2.7 局部和全局结构间的弱对应 . . . 178 
8.2.8 优化的理论限制 . . . . . . . . . . . . . . 179 
8.3 基本算法. . . . . . . . . . . . . . . . . . . . . . . . .180 
8.3.1 随机梯度下降 . . . . . . . . . . . . . . . . 180 
8.3.2 动量 . . . . . . . . . . . . . . . . . . . . . . . . 181 
8.3.3 Nesterov 动量. . . . . . . . . . . . . . . .183
8.4 参数初始化策略 . . . . . . . . . . . . . . . . . 184 
8.5 自适应学习率算法 . . . . . . . . . . . . . . . 187 
8.5.1 AdaGrad . . . . . . . . . . . . . . . . . . . 187 
8.5.2 RMSProp . . . . . . . . . . . . . . . . . . . 188 
8.5.3 Adam . . . . . . . . . . . . . . . . . . . . . . . 189 
8.5.4 选择正确的优化算法. . . . . . . . . .190 
8.6 二阶近似方法 . . . . . . . . . . . . . . . . . . . . 190 
8.6.1 牛顿法 . . . . . . . . . . . . . . . . . . . . . . 190 
8.6.2 共轭梯度 . . . . . . . . . . . . . . . . . . . . 191 
8.6.3 BFGS. . . . . . . . . . . . . . . . . . . . . . . 193 
8.7 优化策略和元算法 . . . . . . . . . . . . . . . 194 
8.7.1 批标准化 . . . . . . . . . . . . . . . . . . . . 194 
8.7.2 坐标下降 . . . . . . . . . . . . . . . . . . . . 196 
8.7.3 Polyak 平均 . . . . . . . . . . . . . . . . . 197 
8.7.4 监督预训练 . . . . . . . . . . . . . . . . . . 197 
8.7.5 设计有助于优化的模型 . . . . . . . 199 
8.7.6 延拓法和课程学习 . . . . . . . . . . . .199 
第 9 章 卷积网络 . . . . . . . . . . . . . . . . . . . . . . . . 201 
9.1 卷积运算. . . . . . . . . . . . . . . . . . . . . . . . .201 
9.2 动机 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203 
9.3 池化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207 
9.4 卷积与池化作为一种无限强的先验 . . . . . . . . . . . . . . . . . . . . . . . 210 
9.5 基本卷积函数的变体. . . . . . . . . . . . .211 
9.6 结构化输出 . . . . . . . . . . . . . . . . . . . . . . 218 
9.7 数据类型. . . . . . . . . . . . . . . . . . . . . . . . .219 
9.8 高效的卷积算法 . . . . . . . . . . . . . . . . . 220 
9.9 随机或无监督的特征. . . . . . . . . . . . .220 
9.10 卷积网络的神经科学基础 . . . . . . . 221 
9.11 卷积网络与深度学习的历史 . . . . 226 
第 10 章 序列建模：循环和递归网络 . . . . 227 
10.1 展开计算图 . . . . . . . . . . . . . . . . . . . . . 228 
10.2 循环神经网络 . . . . . . . . . . . . . . . . . . .230 
10.2.1 导师驱动过程和输出循环网络 232 
10.2.2 计算循环神经网络的梯度 . . . . 233 
10.2.3 作为有向图模型的循环网络 . . 235 
10.2.4 基于上下文的 RNN 序列建模 237 
10.3 双向 RNN . . . . . . . . . . . . . . . . . . . . . . 239 
10.4 基于编码 - 解码的序列到序列架构 . . . . . . . . . . . . . . . . . . . . . . . 240 
10.5 深度循环网络 . . . . . . . . . . . . . . . . . . .242 
10.6 递归神经网络 . . . . . . . . . . . . . . . . . . .243
10.7 长期依赖的挑战 . . . . . . . . . . . . . . . . 244 
10.8 回声状态网络 . . . . . . . . . . . . . . . . . . .245 
10.9 渗漏单元和其他多时间尺度的策略 . . . . . . . . . . . . . . . . . . . . . . 247 
10.9.1 时间维度的跳跃连接. . . . . . . . .247 
10.9.2 渗漏单元和一系列不同时间尺度 . . . . . . . . . . . . . . . . . . . . . . . 247 
10.9.3 删除连接 . . . . . . . . . . . . . . . . . . . 248 
10.10 长短期记忆和其他门控 RNN . 248 
10.10.1 LSTM . . . . . . . . . . . . . . . . . . . . 248 
10.10.2 其他门控 RNN. . . . . . . . . . . . .250 
10.11 优化长期依赖. . . . . . . . . . . . . . . . . .251 
10.11.1 截断梯度 . . . . . . . . . . . . . . . . . . 251 
10.11.2 引导信息流的正则化 . . . . . . . 252 
10.12 外显记忆 . . . . . . . . . . . . . . . . . . . . . . 253 
第 11 章 实践方法论 . . . . . . . . . . . . . . . . . . . . 256 
11.1 性能度量. . . . . . . . . . . . . . . . . . . . . . . .256 
11.2 默认的基准模型 . . . . . . . . . . . . . . . . 258 
11.3 决定是否收集更多数据 . . . . . . . . . 259 
11.4 选择超参数 . . . . . . . . . . . . . . . . . . . . . 259 
11.4.1 手动调整超参数 . . . . . . . . . . . . .259 
11.4.2 自动超参数优化算法. . . . . . . . .262 
11.4.3 网格搜索 . . . . . . . . . . . . . . . . . . . 262 
11.4.4 随机搜索 . . . . . . . . . . . . . . . . . . . 263 
11.4.5 基于模型的超参数优化 . . . . . . 264 
11.5 调试策略. . . . . . . . . . . . . . . . . . . . . . . .264 
11.6 示例：多位数字识别 . . . . . . . . . . . . 267 
第 12 章 应用. . . . . . . . . . . . . . . . . . . . . . . . . . . .269 
12.1 大规模深度学习 . . . . . . . . . . . . . . . . 269 
12.1.1 快速的 CPU 实现 . . . . . . . . . . . 269 
12.1.2 GPU 实现 . . . . . . . . . . . . . . . . . . 269 
12.1.3 大规模的分布式实现. . . . . . . . .271 
12.1.4 模型压缩 . . . . . . . . . . . . . . . . . . . 271 
12.1.5 动态结构 . . . . . . . . . . . . . . . . . . . 272 
12.1.6 深度网络的专用硬件实现 . . . . 273 
12.2 计算机视觉 . . . . . . . . . . . . . . . . . . . . . 274 
12.2.1 预处理 . . . . . . . . . . . . . . . . . . . . . 275 
12.2.2 数据集增强 . . . . . . . . . . . . . . . . . 277 
12.3 语音识别. . . . . . . . . . . . . . . . . . . . . . . .278 
12.4 自然语言处理 . . . . . . . . . . . . . . . . . . .279 
12.4.1 n-gram . . . . . . . . . . . . . . . . . . . . .280 
12.4.2 神经语言模型 . . . . . . . . . . . . . . . 281
12.4.3 高维输出 . . . . . . . . . . . . . . . . . . . 282 
12.4.4 结合 n-gram 和神经语言模型 286 
12.4.5 神经机器翻译 . . . . . . . . . . . . . . . 287 
12.4.6 历史展望 . . . . . . . . . . . . . . . . . . . 289 
12.5 其他应用. . . . . . . . . . . . . . . . . . . . . . . .290 
12.5.1 推荐系统 . . . . . . . . . . . . . . . . . . . 290 
12.5.2 知识表示、推理和回答 . . . . . . . 292 

第 3 部分 深度学习研究 
第 13 章 线性因子模型 . . . . . . . . . . . . . . . . . . 297 
13.1 概率 PCA 和因子分析 . . . . . . . . . . 297 
13.2 独立成分分析 . . . . . . . . . . . . . . . . . . .298 
13.3 慢特征分析 . . . . . . . . . . . . . . . . . . . . . 300 
13.4 稀疏编码. . . . . . . . . . . . . . . . . . . . . . . .301 
13.5 PCA 的流形解释 . . . . . . . . . . . . . . . 304 
第 14 章 自编码器 . . . . . . . . . . . . . . . . . . . . . . . 306 
14.1 欠完备自编码器 . . . . . . . . . . . . . . . . 306 
14.2 正则自编码器 . . . . . . . . . . . . . . . . . . .307 
14.2.1 稀疏自编码器 . . . . . . . . . . . . . . . 307 
14.2.2 去噪自编码器 . . . . . . . . . . . . . . . 309 
14.2.3 惩罚导数作为正则. . . . . . . . . . .309 
14.3 表示能力、层的大小和深度 . . . . . 310 
14.4 随机编码器和解码器. . . . . . . . . . . .310 
14.5 去噪自编码器详解 . . . . . . . . . . . . . . 311 
14.5.1 得分估计 . . . . . . . . . . . . . . . . . . . 312 
14.5.2 历史展望 . . . . . . . . . . . . . . . . . . . 314 
14.6 使用自编码器学习流形 . . . . . . . . . 314 
14.7 收缩自编码器 . . . . . . . . . . . . . . . . . . .317 
14.8 预测稀疏分解 . . . . . . . . . . . . . . . . . . .319 
14.9 自编码器的应用 . . . . . . . . . . . . . . . . 319 
第 15 章 表示学习 . . . . . . . . . . . . . . . . . . . . . . . 321 
15.1 贪心逐层无监督预训练 . . . . . . . . . 322 
15.1.1 何时以及为何无监督预训练有效有效 . . . . . . . . . . . . . . . . . . . 323 
15.2 迁移学习和领域自适应 . . . . . . . . . 326 
15.3 半监督解释因果关系. . . . . . . . . . . .329 
15.4 分布式表示 . . . . . . . . . . . . . . . . . . . . . 332 
15.5 得益于深度的指数增益 . . . . . . . . . 336 
15.6 提供发现潜在原因的线索 . . . . . . . 337
第 16 章 深度学习中的结构化概率模型 . 339 
16.1 非结构化建模的挑战. . . . . . . . . . . .339 
16.2 使用图描述模型结构. . . . . . . . . . . .342 
16.2.1 有向模型 . . . . . . . . . . . . . . . . . . . 342 
16.2.2 无向模型 . . . . . . . . . . . . . . . . . . . 344 
16.2.3 配分函数 . . . . . . . . . . . . . . . . . . . 345 
16.2.4 基于能量的模型 . . . . . . . . . . . . .346 
16.2.5 分离和 d-分离 . . . . . . . . . . . . . . .347 
16.2.6 在有向模型和无向模型中转换 350 
16.2.7 因子图 . . . . . . . . . . . . . . . . . . . . . 352 
16.3 从图模型中采样 . . . . . . . . . . . . . . . . 353 
16.4 结构化建模的优势 . . . . . . . . . . . . . . 353 
16.5 学习依赖关系 . . . . . . . . . . . . . . . . . . .354 
16.6 推断和近似推断 . . . . . . . . . . . . . . . . 354 
16.7 结构化概率模型的深度学习方法. . . . . . . . . . . . . . . . . . . . . . . . .355 
16.7.1 实例：受限玻尔兹曼机 . . . . . . . 356 
第 17 章 蒙特卡罗方法 . . . . . . . . . . . . . . . . . . 359 
17.1 采样和蒙特卡罗方法. . . . . . . . . . . .359 
17.1.1 为什么需要采样 . . . . . . . . . . . . .359 
17.1.2 蒙特卡罗采样的基础. . . . . . . . .359 
17.2 重要采样. . . . . . . . . . . . . . . . . . . . . . . .360 
17.3 马尔可夫链蒙特卡罗方法 . . . . . . . 362 
17.4 Gibbs 采样. . . . . . . . . . . . . . . . . . . . . .365 
17.5 不同的峰值之间的混合挑战 . . . . 365 
17.5.1 不同峰值之间通过回火来混合 367 
17.5.2 深度也许会有助于混合 . . . . . . 368 
第 18 章 直面配分函数 . . . . . . . . . . . . . . . . . . 369 
18.1 对数似然梯度 . . . . . . . . . . . . . . . . . . .369 
18.2 随机最 大似然和对比散度 . . . . . . . 370 
18.3 伪似然 . . . . . . . . . . . . . . . . . . . . . . . . . . 375 
18.4 得分匹配和比率匹配. . . . . . . . . . . .376 
18.5 去噪得分匹配 . . . . . . . . . . . . . . . . . . .378 
18.6 噪声对比估计 . . . . . . . . . . . . . . . . . . .378 
18.7 估计配分函数 . . . . . . . . . . . . . . . . . . .380 
18.7.1 退火重要采样 . . . . . . . . . . . . . . . 382 
18.7.2 桥式采样 . . . . . . . . . . . . . . . . . . . 384 
第 19 章 近似推断 . . . . . . . . . . . . . . . . . . . . . . . 385 
19.1 把推断视作优化问题. . . . . . . . . . . .385 
19.2 期望最 大化 . . . . . . . . . . . . . . . . . . . . . 386 
19.3 最 大后验推断和稀疏编码 . . . . . . . 387
19.4 变分推断和变分学习. . . . . . . . . . . .389 
19.4.1 离散型潜变量 . . . . . . . . . . . . . . . 390 
19.4.2 变分法 . . . . . . . . . . . . . . . . . . . . . 394 
19.4.3 连续型潜变量 . . . . . . . . . . . . . . . 396 
19.4.4 学习和推断之间的相互作用 . . 397 
19.5 学成近似推断 . . . . . . . . . . . . . . . . . . .397 
19.5.1 醒眠算法 . . . . . . . . . . . . . . . . . . . 398 
19.5.2 学成推断的其他形式. . . . . . . . .398 
第 20 章 深度生成模型 . . . . . . . . . . . . . . . . . . 399 
20.1 玻尔兹曼机 . . . . . . . . . . . . . . . . . . . . . 399 
20.2 受限玻尔兹曼机 . . . . . . . . . . . . . . . . 400 
20.2.1 条件分布 . . . . . . . . . . . . . . . . . . . 401 
20.2.2 训练受限玻尔兹曼机. . . . . . . . .402 
20.3 深度信念网络 . . . . . . . . . . . . . . . . . . .402 
20.4 深度玻尔兹曼机 . . . . . . . . . . . . . . . . 404 
20.4.1 有趣的性质 . . . . . . . . . . . . . . . . . 406 
20.4.2 DBM 均匀场推断 . . . . . . . . . . . 406 
20.4.3 DBM 的参数学习 . . . . . . . . . . . 408 
20.4.4 逐层预训练 . . . . . . . . . . . . . . . . . 408 
20.4.5 联合训练深度玻尔兹曼机 . . . . 410 
20.5 实值数据上的玻尔兹曼机 . . . . . . . 413 
20.5.1 Gaussian-Bernoulli RBM . . . . 413 
20.5.2 条件协方差的无向模型 . . . . . . 414 
20.6 卷积玻尔兹曼机 . . . . . . . . . . . . . . . . 417 
20.7 用于结构化或序列输出的玻尔兹曼机 . . . . . . . . . . . . . . . . . . . . 418 
20.8 其他玻尔兹曼机 . . . . . . . . . . . . . . . . 419 
20.9 通过随机操作的反向传播 . . . . . . . 419 
20.9.1 通过离散随机操作的反向传播 420 
20.10 有向生成网络. . . . . . . . . . . . . . . . . .422 
20.10.1 sigmoid 信念网络 . . . . . . . . . . 422 
20.10.2 可微生成器网络 . . . . . . . . . . . .423 
20.10.3 变分自编码器 . . . . . . . . . . . . . .425 
20.10.4 生成式对抗网络 . . . . . . . . . . . .427 
20.10.5 生成矩匹配网络 . . . . . . . . . . . .429 
20.10.6 卷积生成网络 . . . . . . . . . . . . . .430 
20.10.7 自回归网络 . . . . . . . . . . . . . . . . 430 
20.10.8 线性自回归网络 . . . . . . . . . . . .430 
20.10.9 神经自回归网络 . . . . . . . . . . . .431 
20.10.10 NADE . . . . . . . . . . . . . . . . . . . 432 
20.11 从自编码器采样 . . . . . . . . . . . . . . . 433
20.11.1 与任意去噪自编码器相关的马尔可夫链 . . . . . . . . . . . . . . . . 434 
20.11.2 夹合与条件采样 . . . . . . . . . . . .434 
20.11.3 回退训练过程 . . . . . . . . . . . . . .435 
20.12 生成随机网络. . . . . . . . . . . . . . . . . .435 
20.12.1 判别性 GSN . . . . . . . . . . . . . . . 436 
20.13 其他生成方案. . . . . . . . . . . . . . . . . .436 
20.14 评估生成模型. . . . . . . . . . . . . . . . . .437 
20.15 结论 . . . . . 438 
参考文献. . . . . . . . . . . . .439 
索引 . . . . . . . . . . . . . . . . . 486

                                 --此文字指  版本。